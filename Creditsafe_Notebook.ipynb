{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce03b895-0d47-4338-9d0a-90d6beb8b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: pdf2image in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdf2image) (11.0.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opencv-python) (1.23.5)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytesseract) (11.0.0)\n",
      "Requirement already satisfied: langdetect in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Requirement already satisfied: nltk in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: thefuzz in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in c:\\users\\patrick\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from thefuzz) (3.10.1)\n"
     ]
    }
   ],
   "source": [
    "# Install the required dependencies\n",
    "# Additional Software Prerequisites: Tesseract, Ghostscript\n",
    "!pip install numpy\n",
    "!pip install pdf2image\n",
    "!pip install opencv-python\n",
    "!pip install pytesseract\n",
    "!pip install langdetect\n",
    "!pip install huggingface_hub\n",
    "!pip install nltk\n",
    "!pip install thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8cf3e112-74b6-4e28-a8a3-80ba6b71d31d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## BASIC SETUP ##\n",
    "\n",
    "# Default modules\n",
    "import tempfile\n",
    "import glob # Pathname management\n",
    "import os\n",
    "import string\n",
    "import re # Regex\n",
    "import json\n",
    "import getpass # Passkey management\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "import pdf2image # PDF handling\n",
    "import pytesseract as tess # OCR\n",
    "import cv2 # Image processing\n",
    "import nltk # Natural language processing\n",
    "import langdetect\n",
    "import huggingface_hub # Hugging Face interface\n",
    "from thefuzz import process as fuzzProcess # Fuzzy strng comparison\n",
    "\n",
    "# Set up the directories and file structure\n",
    "working_dir = os.getcwd()\n",
    "input_dir = working_dir + \"\\\\data\\\\\" # Location of the PDF files to be scanned\n",
    "int_dir = working_dir + \"\\\\intermediate\\\\\" # Used for storing intermediate results\n",
    "output_dir = working_dir + \"\\\\output\\\\\" # Used for storing final output\n",
    "\n",
    "# Make the output paths if they don't already exist\n",
    "try:\n",
    "    os.mkdir(int_dir)\n",
    "    os.mkdir(output_dir)\n",
    "except:\n",
    "    None\n",
    "    \n",
    "# Collect all the filenames in the input directory\n",
    "documents = glob.glob(input_dir + \"*.pdf\")\n",
    "    \n",
    "# Final dictionary that will hold results and be written to JSON\n",
    "final_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1717bf17-1daa-49c4-97e8-323b992bd237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## HUGGING FACE INITIALIZATION ##\n",
    "\n",
    "# Get the Hugging Face API Key\n",
    "try:\n",
    "    key = os.environ[\"HUGGINGFACE_API_TOKEN\"]\n",
    "except KeyError:\n",
    "    key = getpass.getpass(\"Enter Hugging Face API Key\")\n",
    "    \n",
    "# Define the models being used for various tasks\n",
    "translation_model = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "summarization_model = \"facebook/bart-large-cnn\"\n",
    "#questioning_model = \"deepset/roberta-base-squad2\"\n",
    "questioning_model = \"distilbert/distilbert-base-cased-distilled-squad\"\n",
    "token_classification_model = \"Babelscape/wikineural-multilingual-ner\"\n",
    "    \n",
    "# Set up the Hugging Face clients\n",
    "client = huggingface_hub.InferenceClient(token = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8756b60-30a0-4e0e-ad41-8fbcc712edf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## UTILITY FUNCTIONS ##\n",
    "\n",
    "# Converts a language code extracted from the detectlang function to the format required for the BART translation model\n",
    "def BART_lang_code(lang):\n",
    "    if lang == 'en':\n",
    "        return 'en_XX'\n",
    "    if lang == 'fr':\n",
    "        return 'fr_XX'\n",
    "    elif lang == 'de':\n",
    "        return 'de_DE'\n",
    "    elif lang == 'nl':\n",
    "        return 'nl_XX'\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected language {lang} detected.\")\n",
    "    \n",
    "# Converts a language code extracted from the detectlang function to the format required for the NLTK Punkt algorithm\n",
    "def PUNKT_lang_code(lang):\n",
    "    if lang == 'en':\n",
    "        return 'english'\n",
    "    elif lang == 'fr':\n",
    "        return 'french'\n",
    "    elif lang == 'de':\n",
    "        return 'german'\n",
    "    elif lang == 'nl':\n",
    "        return 'dutch'\n",
    "    else:\n",
    "        raise Exception(f\"Unexpected language {lang} detected.\")\n",
    "\n",
    "# Converts an image from Pillow format (output by pdf2img) to OpenCV format\n",
    "def PIL_to_CV2(img):\n",
    "    return cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Crops a rectangular region defined between the points (left, top) and (right, bottom) out of an OpenCV image\n",
    "def crop_img(img, left, top, right, bottom):\n",
    "    return img[top : bottom, left : right]\n",
    "\n",
    "# Searches an OpenCV image, img, vertically for rectangular regions of empty space, from top to bottom, and returns the midpoint of the first rectangle encountered\n",
    "# The rectangle is defined by the coordinates \n",
    "#     (top, left)     = (divOffsetY, divOffsetLeft) \n",
    "#     (bottom, right) = (divOffsetY + divHeight, width - divOffsetRight)\n",
    "# divOffsetY is continually incremented and the rectangular region checked for emptiness, with the first valid match triggering a return\n",
    "def vertical_sliding_split_img(img, divOffsetY = 0, divOffsetLeft = 0, divOffsetRight = 0, divHeight = 20):\n",
    "    # Get the dimensions of the image to be searched\n",
    "    height, width = img.shape[:2]\n",
    "        \n",
    "    # Slide the rectangular region down the image\n",
    "    while divOffsetY + divHeight <= height:\n",
    "        # Crop out the area of the original image that corresponds to the rectangle\n",
    "        mask = crop_img(img, left = divOffsetLeft, top = divOffsetY, right = (width - divOffsetRight), bottom = (divOffsetY + divHeight))\n",
    "\n",
    "        # Check if it is empty\n",
    "        if cv2.countNonZero(mask) == 0:\n",
    "            # Get the centre of the location of the masking rectangle and return it\n",
    "            return divOffsetY + int(divHeight / 2)\n",
    "        # If non-empty, slide the rectangle further down\n",
    "        else:\n",
    "            divOffsetY += 1\n",
    "\n",
    "    # If we find no matches, raise an error\n",
    "    if divOffsetY + divHeight == height:\n",
    "        raise Exception(\"No valid split found\")\n",
    "\n",
    "# Function to search for the first found regex result in a block of text, extract the result, and return the result and all the text immediately after it\n",
    "def rgx_and_crop(text, rgx):\n",
    "    # Search the text\n",
    "    search = re.search(rgx, text) \n",
    "\n",
    "    # If a match was found\n",
    "    if search:\n",
    "        # Get the index of the final character of the (first) correct match\n",
    "        clipLeft = search.end(0)\n",
    "         \n",
    "        # Extract the match and clean-up the string\n",
    "        search = search.group(0).strip().upper()\n",
    "\n",
    "        # Clip the original text up to the final character and remove any additional trialing whitespace\n",
    "        text = text[clipLeft + 1:].strip()\n",
    "\n",
    "        # Return both the result and the clipped text\n",
    "        return(search, text)\n",
    "    \n",
    "    # Otherwise return a blank string\n",
    "    else:\n",
    "        return (\"\", text)\n",
    "        \n",
    "# Simple function to clean any leading or trailing whitespace or punctuation from a string\n",
    "def clean_string(s):\n",
    "    return s.strip(string.whitespace + string.punctuation)\n",
    "\n",
    "# Given a list of tokens (e.g. names of people) this function attempts to identify common errors such as duplicates and misspellings and return a cleaned list of tokens removing such errors\n",
    "# Takes an iterable of strings as input\n",
    "def clean_token_list(tokenList):\n",
    "    # Start by cleaning the strings\n",
    "    tokenList = [clean_string(t) for t in tokenList]\n",
    "    \n",
    "    # Eliminate duplicates by converting the list to a set (and back again to preserve ordering going forward)\n",
    "    tokenList = list(set(tokenList))\n",
    "    \n",
    "    # Sort them by length from shortest to longest\n",
    "    tokenList.sort(reverse = True, key = lambda x : len(x))\n",
    "    \n",
    "    # Eliminate any shortened or cropped names by removing strings that are substings of any larger strings\n",
    "    # e.g. if we have \"David McArthur\" and \"David McAr\" we make the assumption that \"David McAr\" is meant to read \"David McArthur\"\n",
    "    tokenList = [t for t in tokenList if all([(t not in s) for s in tokenList if s != t])]\n",
    "    \n",
    "    # Perform fuzzy matching to determine any close matches\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tokenList):\n",
    "        # For each word in the token list, calculate the (Levenshtein) distance between all the other words on the token list using TheFuzz\n",
    "        matches = fuzzProcess.extract(tokenList[i], tokenList)\n",
    "        \n",
    "        # Pop the first result since this will always be an identical match\n",
    "        matches.pop(0)\n",
    "        \n",
    "        # For the remaining possible matches, iterate through the pairs of (text, ratio)\n",
    "        for matchText, matchRatio in matches:\n",
    "            \n",
    "            # If there are any very close matches (ratio > 95), pop the match from the tokenList and advance the iterator and loop\n",
    "            if matchRatio > 95:\n",
    "                tokenList.remove(matchText)\n",
    "                i += 1\n",
    "                continue # Break the current loop\n",
    "                \n",
    "        # Advance the iterator always, even if we remove no tokens\n",
    "        i += 1\n",
    "    \n",
    "    return tokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "da1d4396-bc12-4210-91a7-1061a93b359c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## MAIN PROGRAM ##\n",
    "\n",
    "# Create a temporary directory to store converted PDFs\n",
    "def process_PDFs(debug = False):\n",
    "    final_dict = dict()\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        for doc in documents:\n",
    "            print(f\"Extracting text from {doc}\")\n",
    "\n",
    "            # Extract just the file name for later reuse\n",
    "            working_filename = os.path.basename(doc).split('.')[0]\n",
    "\n",
    "            # Start by converting the pdf to an image and store the resulting PIL image\n",
    "            img = pdf2image.convert_from_path(doc, output_folder = temp_dir)[0]\n",
    "\n",
    "            # Convert the image from Pillow to CV2 format using our helper function\n",
    "            img = PIL_to_CV2(img)\n",
    "\n",
    "            # Get the dimensions\n",
    "            height, width = img.shape[:2]\n",
    "\n",
    "            # IMAGE PREPROCESSING #\n",
    "\n",
    "            # Based on analysing the documents manually we can safely perform the following rough, initial crop\n",
    "            img = crop_img(img, left = 275, top = 475, right = width - 100, bottom = height - 150)\n",
    "\n",
    "            # Convert to grayscale\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Invert the colours\n",
    "            img = cv2.bitwise_not(img)\n",
    "\n",
    "            # Close any small holes\n",
    "            img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_RECT, (3,3)))\n",
    "\n",
    "            # Downsize the image, this helps the OCR perform better\n",
    "            img = cv2.resize(img, (int(0.5*width), int(0.5*height))) \n",
    "\n",
    "            # Update the width and height\n",
    "            height, width = img.shape[:2]\n",
    "\n",
    "            # Threshold\n",
    "            # img = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY)[1]# + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "            if debug:\n",
    "                # Write the image\n",
    "                cv2.imwrite(int_dir + working_filename + '.jpg', img)\n",
    "\n",
    "            # Split the image into upper and lower halves by finding large, black, rectangular regions and taking the centroid of the first (from top to bottom) as the dividing line  \n",
    "            # See the Data Assumptions section of the readme\n",
    "            divY = vertical_sliding_split_img(img, divOffsetY = 0, divOffsetLeft = 20, divOffsetRight = 55, divHeight = 20)\n",
    "            upperImg = img[:divY, :]\n",
    "            lowerImg = img[divY:, :]\n",
    "\n",
    "            # Further split the lower section into a middle and lower section    \n",
    "            divY = vertical_sliding_split_img(img, divOffsetY = 40, divOffsetLeft = 60, divOffsetRight = 60, divHeight = 5)\n",
    "            middleImg = lowerImg[:divY, :]\n",
    "            lowerImg = lowerImg[divY:, :]\n",
    "\n",
    "            # Write if debugging\n",
    "            if debug:\n",
    "                cv2.imwrite(f\"{int_dir}{working_filename}_upper.jpg\", upperImg)\n",
    "                cv2.imwrite(f\"{int_dir}{working_filename}_middle.jpg\", middleImg)\n",
    "                cv2.imwrite(f\"{int_dir}{working_filename}_lower.jpg\", lowerImg)\n",
    "\n",
    "            # Process the lower, largest section first, since this is also the easiest to extract the language of the text from with no additional image processing\n",
    "            # These functions are defined in the following code blocks\n",
    "            document_info, lang = process_lower(lowerImg, output_filename = working_filename, debug = debug)\n",
    "\n",
    "            # Then process the middle annd upper sections\n",
    "            document_type = process_middle(middleImg, lang, output_filename = working_filename, debug = debug)\n",
    "\n",
    "            company_info = process_upper(upperImg, lang, output_filename = working_filename, debug = debug)\n",
    "\n",
    "            # Merge the dictionaries\n",
    "            doc_dict = company_info | document_type | document_info\n",
    "\n",
    "            # Add the dictionary to the global document dictionary\n",
    "            final_dict[working_filename] = doc_dict\n",
    "            \n",
    "            if debug:\n",
    "                with open(f'{int_dir}{working_filename}.json', 'w', encoding = 'utf-8') as file:\n",
    "                    file.write(json.dumps(final_dict, indent = 4))                \n",
    "            \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3c2750cb-38be-489a-89ee-68a9063d5bd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes an image of the lowest section of the document (the main body of text) and processes it using OCR and Gen AI\n",
    "# Returns a dictionary containing the names of people mentioned in the document, their roles (if any), and a short summary of the document\n",
    "# Also returns the detected language of the document\n",
    "def process_lower(img, output_filename = \"\", debug = False):\n",
    "    document_info = dict()\n",
    "    \n",
    "    # Process the text using Tesseract    \n",
    "    text = tess.image_to_string(img, lang = 'fra+nld+deu+eng', config = r'--psm 3')\n",
    "\n",
    "    # Detect the most probabalistic language\n",
    "    lang = langdetect.detect(text)    \n",
    "    \n",
    "    # Write the raw OCR text if debugging\n",
    "    if debug:\n",
    "        # Write the raw OCR'd text to file for debugging\n",
    "        with open(f'{int_dir}{output_filename}_lower_{lang}.txt', 'w', encoding = 'utf-8') as file:\n",
    "            file.write(text)\n",
    "        \n",
    "    # Due to token limits with the translation we need to process the translation in smaller chunks\n",
    "    # Sentence-by-sentence makes the most sense, and we can do this using nltk\n",
    "    sentences = nltk.sent_tokenize(text, language = PUNKT_lang_code(lang))\n",
    "    \n",
    "    # Translate the text to English sentence-by-sentence and rejoin them as a string\n",
    "    text = \" \".join([client.translation(s, model = translation_model, src_lang = BART_lang_code(lang), tgt_lang = 'en_XX').translation_text for s in sentences])\n",
    "    \n",
    "    # Write the translated text if debugging\n",
    "    if debug:\n",
    "        with open(f'{int_dir}{output_filename}_lower_en.txt', 'w', encoding = 'utf-8') as file:\n",
    "            file.write(text)\n",
    "    \n",
    "    # Determine the people mentioned in the text using token classification\n",
    "    classified_tokens = client.token_classification(text, model = token_classification_model)\n",
    "    \n",
    "    # Separate out the tokens corresponding to people's names\n",
    "    people_tokens = [c['word'] for c in classified_tokens if c['entity_group'] == 'PER']\n",
    "    \n",
    "    # Clean the list\n",
    "    people_tokens = clean_token_list(people_tokens)\n",
    "    \n",
    "    # Define an empty dictionary\n",
    "    # We store people in the dictionary with their names as the key, and their roles as the value\n",
    "    people_dict = dict()\n",
    "    \n",
    "    # For each person we want to determine if they have a particular role using the question-asking model\n",
    "    # We do this in the context of the body of the text\n",
    "    for person in people_tokens:\n",
    "        Q_ppl = f'What role does {person} have?'\n",
    "        Q_ppl_ans = client.question_answering(question = Q_ppl, model = questioning_model, context = text, doc_stride = 350)\n",
    "        \n",
    "        # If the answer is high-confidence, record it\n",
    "        if Q_ppl_ans.score >= 0.9:\n",
    "            people_dict[person.title()] = Q_ppl_ans.answer.title()\n",
    "        # Otherwise list unknonw\n",
    "        else:\n",
    "            people_dict[person.title()] = 'Unknown'\n",
    "            \n",
    "    # Store the dictionary\n",
    "    document_info['people'] = people_dict\n",
    "    \n",
    "    # Create the short summary using the summarization model\n",
    "    text_summary = client.summarization(text, model = summarization_model).summary_text #, parameters = {\"max_length\" : 250, \"min_length\" : 30, \"do_sample\" : False}).summary_text\n",
    "    document_info['summary'] = text_summary    \n",
    "    \n",
    "    return (document_info, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b12b0a46-871b-4f54-ab03-f872bd2ca74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This middle segment of the image is a single line of text corresponding to the Purpose of the Act\n",
    "def process_middle(img, lang = 'fr', output_filename = \"\", debug = False):   \n",
    "    # We should only be dealing with a single line of text so we use Tesseract with the --psm 11 option to maximize extraction of raw text\n",
    "    purposeText = clean_string(tess.image_to_string(img, lang = 'fra+nld+deu+eng', config = r'--psm 11'))\n",
    "    \n",
    "    # Write the text if debugging\n",
    "    if debug:\n",
    "        with open(f'{int_dir}{output_filename}_middle_{lang}.txt', 'w', encoding = 'utf-8') as file:\n",
    "            file.write(purposeText)\n",
    "    \n",
    "    # We want to slice the middle text using the position of any found colons\n",
    "    # We include a few similar characters to match in the regex which a colon may be incorrectly detected as\n",
    "    rgx = r'(?s)(:|‘|.|\\'|`)(.*){2,}'\n",
    "    purposeText = clean_string(re.search(rgx, purposeText).group(0))\n",
    "    \n",
    "    # Translate the purpose and store it\n",
    "    purposeText = client.translation(purposeText, model = translation_model, src_lang = BART_lang_code(lang), tgt_lang = 'en_XX').translation_text\n",
    "    \n",
    "    # Write the translated text if debugging\n",
    "    if debug:\n",
    "        with open(f'{int_dir}{output_filename}_middle_en.txt', 'w', encoding = 'utf-8') as file:\n",
    "            file.write(purposeText)\n",
    "    \n",
    "    return {'purpose' : purposeText.title()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "801b326c-528d-4665-bea8-62974a6efb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The upper segment of the image consists of a table-like structure with headers on the left and information on the right\n",
    "# This information is assumed to always be of the same structure, vertically descent as follow\n",
    "# - Company identifier\n",
    "# - Company name\n",
    "# - Company type\n",
    "# - Comapny address\n",
    "# The division between the headers and the information is determined by locating all the scolons in the image and taking the median of the position\n",
    "# We take the median because the address section can be inconsistent with its location of any colons\n",
    "\n",
    "def process_upper(img, lang = 'fr', output_filename = \"\", debug = False):     \n",
    "    company_info = dict()\n",
    "    \n",
    "    # We perform an initial OCR using pytesseract to detect any colons in particular\n",
    "    # Dilate the image first slightly since these colons may be particularly small and easy-to-miss with OCR detection\n",
    "    colon_img = cv2.dilate(img, kernel = np.ones((2,2)))\n",
    "    \n",
    "    # Perform the OCR\n",
    "    ocr_tree = tess.image_to_data(colon_img, lang = 'fra+nld+deu+eng', output_type = tess.Output.DATAFRAME, config = r'--psm 3')\n",
    "    \n",
    "    # Detect the locations of the colons and take the median value (see above for reasoning)\n",
    "    # Apply a slight pixel offset so the position is actually to the right and clears the colons\n",
    "    semi_loc = int(ocr_tree[ocr_tree['text'].str.contains(r':', na = False)]['left'].median()) + 5\n",
    "\n",
    "    # Slice and preserve everything right of the colons \n",
    "    height, width = img.shape[:2]\n",
    "    img = crop_img(img, left = semi_loc, top = 0, right = width, bottom = height)\n",
    "    \n",
    "    # Write the image if debugging\n",
    "    if debug:\n",
    "        cv2.imwrite(int_dir + output_filename + '_upper.jpg', img)\n",
    "    \n",
    "    # Perform a secondary OCR on the cropped image\n",
    "    # Note the use of --psm 11 to extract as much raw text as possible\n",
    "    text = tess.image_to_string(img, lang = 'fra+nld+deu+eng', config = r'--psm 11') \n",
    "\n",
    "    # Attempt to extract the company identifier using regex to pick out strings of consecutive digits with spaces or hyphens between them\n",
    "    identifier_rgx = r\"([0-9])([0-9|-| ]){1,}\"\n",
    "    identifier, text = rgx_and_crop(text, identifier_rgx)\n",
    "    \n",
    "    # Remove all whitespaces and dashes\n",
    "    identifier = identifier.replace(\" \", \"\")\n",
    "    identifier = identifier.replace(\"-\", \"\")\n",
    "    \n",
    "    # Clean the ends\n",
    "    company_info[\"identifier\"] = clean_string(identifier)\n",
    "\n",
    "    # Use a similar technique for the company name by detecting the next textual string up to a newline character\n",
    "    text_rgx = r'([0-9|A-Z|a-z|À-ÿ])(.*){2,}'\n",
    "    name, text = rgx_and_crop(text, text_rgx)\n",
    "    \n",
    "    # Clean the ends\n",
    "    company_info[\"name\"] = clean_string(name).title()\n",
    "\n",
    "    # Repeat this techniue for the company type\n",
    "    company_type, text = rgx_and_crop(text, text_rgx)\n",
    "    \n",
    "    # Clean the string\n",
    "    company_type = clean_string(company_type)\n",
    "    \n",
    "    # For this data we also want to translate it\n",
    "    company_type = client.translation(company_type, model = translation_model, src_lang = BART_lang_code(lang), tgt_lang = 'en_XX').translation_text\n",
    "    company_info[\"company type\"]  = company_type.title()\n",
    "\n",
    "    # Repeat the process again for the address\n",
    "    address, text = rgx_and_crop(text, text_rgx)\n",
    "    \n",
    "    # Clean the ends and fix the formatting\n",
    "    company_info[\"address\"] = clean_string(address).title()\n",
    "    \n",
    "    return company_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1594a-c887-4ec8-8db3-31087aef006f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000001.pdf\n",
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000002.pdf\n",
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000003.pdf\n",
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000004.pdf\n",
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000005.pdf\n",
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000006.pdf\n",
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000007.pdf\n",
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000008.pdf\n",
      "Extracting text from g:\\Sync\\Work\\Maths\\Jupyter_Notebooks\\Creditsafe Interview Task\\data\\24000009.pdf\n"
     ]
    }
   ],
   "source": [
    "json_data = process_PDFs(debug = False)\n",
    "\n",
    "# Write to final output JSON\n",
    "with open(f'{output_dir}output.json', 'w', encoding = 'utf-8') as file:\n",
    "    file.write(json.dumps(json_data, indent = 4))\n",
    "    print(f\"{output_dir}output.json written\")\n",
    "    \n",
    "print(\"Complete\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
